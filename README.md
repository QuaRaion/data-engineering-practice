# Практические работы по Docker, Neo4j и Apache Airflow

Практические работы по освоению Docker, Neo4j и Apache Airflow для создания эффективных ETL-пайплайнов и управления данными. Включает примеры настройки контейнеров, работы с графовыми базами данных и оркестрации задач

## Содержание работ

### 1. **Установка и настройка Docker**
   В этой работе было произведено развертывание Docker на локальной машине, настройка необходимых контейнеров для работы с различными сервисами, включая базы данных и инструменты для автоматизации. Работа включает в себя создание конфигурации `docker-compose.yml` для упрощения управления сервисами и взаимодействия с ними, а также настройку volumes для сохранения данных при перезапуске контейнеров.

### 2. **Управление данными в БД Neo4j**
   В ходе выполнения данной работы был изучен подход к управлению графовыми данными с использованием базы данных Neo4j. Рассмотрены основные операции с графами, такие как создание узлов и рёбер, а также выполнение запросов на языке Cypher для извлечения и обновления информации в графах. Работа включает практическое создание и модификацию графов для решения типичных задач в области анализа данных.

### 3. **Подключение к Neo4j с помощью Python**
   В этой работе было установлено подключение к базе данных Neo4j с использованием Python и библиотеки `neo4j`. Рассмотрены основные методы для взаимодействия с графовой базой данных, включая создание соединений, выполнение запросов и обработку результатов. Также изучено использование Python для автоматизации операций с базой данных и интеграции с другими инструментами.

### 4. **Знакомство с функционалом Apache Airflow**
   В рамках этой работы был изучен Apache Airflow, инструмент для оркестрации рабочих процессов и автоматизации задач. Рассмотрено создание простых DAG файлов для выполнения различных операций, таких как запуск задач, их зависимость и планирование. Также был изучен веб-интерфейс Airflow для мониторинга и управления выполнением задач.

### 5. **Реализация ETL процессов в Apache Airflow**
   В этой работе была реализована сложная автоматизация процессов обработки данных с использованием Apache Airflow. Рассмотрен процесс построения ETL пайплайнов для извлечения, трансформации и загрузки данных. Были созданы DAG файлы для получения данных из внешних API (например, OpenWeather и VK API), их обработки и сохранения в базу данных PostgreSQL и в другие форматы файлов (CSV, JSON, Parquet). Работа включает разработку и настройку задач в Airflow, управление ошибками и оптимизацию процессов.

---

Каждая из этих работ дает практическое понимание и навыки работы с ключевыми инструментами для разработки и автоматизации обработки данных, включая настройку и использование контейнеров Docker, работу с графовыми базами данных Neo4j и автоматизацию процессов с Apache Airflow.
